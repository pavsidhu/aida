{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality Analysis using a Bimodel LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: skorch in /home/ubuntu/.local/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.7/site-packages (2.5.1)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/ubuntu/.local/lib/python3.7/site-packages (from skorch) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ubuntu/.local/lib/python3.7/site-packages (from skorch) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/ubuntu/.local/lib/python3.7/site-packages (from skorch) (0.22.2)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in /home/ubuntu/.local/lib/python3.7/site-packages (from skorch) (4.43.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /home/ubuntu/.local/lib/python3.7/site-packages (from skorch) (0.8.6)\n",
      "Requirement already satisfied: boto3 in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (1.12.11)\n",
      "Requirement already satisfied: sacremoses in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (2020.2.20)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ubuntu/.local/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ubuntu/.local/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ubuntu/.local/lib/python3.7/site-packages (from scikit-learn>=0.19.1->skorch) (0.14.1)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.11 in /home/ubuntu/.local/lib/python3.7/site-packages (from boto3->transformers) (1.15.11)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ubuntu/.local/lib/python3.7/site-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ubuntu/.local/lib/python3.7/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers) (1.10.0)\n",
      "Requirement already satisfied: click in /home/ubuntu/.local/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/.local/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/.local/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/.local/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/lib/python3/dist-packages (from botocore<1.16.0,>=1.15.11->boto3->transformers) (0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch skorch transformers pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skorch\n",
    "import torch\n",
    "\n",
    "from IPython.display import display\n",
    "from skorch import NeuralNet\n",
    "from torch import nn, optim, tensor\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "from model.LstmModel import LstmModel\n",
    "from utils import progress_bar\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the MyPersonality dataset\n",
    "MY_PERSONALITY_PATH = \"data/mypersonality.csv\"\n",
    "\n",
    "# Path to the Essays dataset\n",
    "ESSAYS_PATH = \"data/essays.csv\"\n",
    "\n",
    "# List of traits to analyse\n",
    "TRAITS = [\"cAGR\", \"cCON\", \"cEXT\", \"cOPN\", \"cNEU\"]\n",
    "\n",
    "# Max length of tokens when calculating embeddings\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "# Specify what dataset to use, can be either ESSAYS or MY_PERSONALITY\n",
    "DATASET_TO_USE = \"ESSAYS\"\n",
    "\n",
    "# Specify was model from the Transformers library to use to calculate embeddings\n",
    "EMBEDDINGS_MODEL = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from utils import progress_bar\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Used to process the selected dataset for training and validation\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels):\n",
    "        self.embeddings = []\n",
    "        self.embeddings_lengths = []\n",
    "        self.generate_embeddings(texts)\n",
    "        \n",
    "        self.embeddings_lengths = torch.tensor(self.embeddings_lengths)\n",
    "        self.labels = [\n",
    "            torch.tensor([label.cAGR, label.cCON, label.cEXT, label.cOPN, label.cNEU])\n",
    "            for label in labels.itertuples()\n",
    "        ]\n",
    "\n",
    "    def generate_embeddings(self, texts):\n",
    "        \"\"\"Generate word embeddings for all the texts\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            progress = display(progress_bar(0, 100), display_id=True)\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(EMBEDDINGS_MODEL)\n",
    "\n",
    "            embeddings_model = AutoModel.from_pretrained(EMBEDDINGS_MODEL).to(device)\n",
    "            embeddings_model.eval()\n",
    "        \n",
    "            for i, text in enumerate(texts):\n",
    "                encoded_text = tokenizer.encode_plus(\n",
    "                    text.lower(),\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=MAX_LENGTH,\n",
    "                    pad_to_max_length=True,\n",
    "                )\n",
    "                input_ids = encoded_text[\"input_ids\"]\n",
    "                attention_mask = encoded_text[\"attention_mask\"]\n",
    "\n",
    "                input = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "                input_mask = torch.tensor(attention_mask).to(device).unsqueeze(0)\n",
    "                \n",
    "                output = embeddings_model(input, attention_mask=input_mask)[0]\n",
    "                output = output.squeeze().to(\"cpu\")\n",
    "\n",
    "                self.embeddings.append(output)\n",
    "                self.embeddings_lengths.append(len(input_ids))\n",
    "\n",
    "                progress.update(progress_bar(i, len(texts)))\n",
    "            \n",
    "\n",
    "    def set_trait(self, trait):\n",
    "        \"\"\"Set the trait to use when fetching labels\"\"\"\n",
    "        i = TRAITS.index(trait)\n",
    "        self.trait_index = i\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Get labels from the dataset given a selected trait\"\"\"\n",
    "        return torch.tensor([label[self.trait_index] for label in self.labels])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            self.embeddings[i],\n",
    "            self.embeddings_lengths[i].item(),\n",
    "            self.labels[i][self.trait_index],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Loads a dataset and returns a PyTorch Dataset with embeddings\"\"\"\n",
    "\n",
    "    if DATASET_TO_USE == \"MY_PERSONALITY\":\n",
    "        path = MY_PERSONALITY_PATH\n",
    "        text_field = \"STATUS\"\n",
    "    elif DATASET_TO_USE == \"ESSAYS\":\n",
    "        path = ESSAYS_PATH\n",
    "        text_field = \"TEXT\"\n",
    "\n",
    "    df = pandas.read_csv(path, encoding=\"latin1\")\n",
    "    df[TRAITS] = df[TRAITS].replace(to_replace=[\"y\",\"n\"], value=[1.0,0.0])\n",
    "    df[TRAITS] = df[TRAITS]\n",
    "    df = df.rename(columns={text_field: \"TEXT\"})\n",
    "\n",
    "    return Dataset(df[\"TEXT\"], df[TRAITS])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pads the dataset so tokens are the same length\"\"\"\n",
    "\n",
    "    (embeddings, embeddings_lengths, labels) = zip(*batch)\n",
    "\n",
    "    embeddings_lengths, perm_indexes = torch.tensor(embeddings_lengths).sort(0, descending=True)\n",
    "    \n",
    "    embeddings = torch.stack(embeddings, dim=0)[perm_indexes]\n",
    "    labels = torch.tensor(labels)[perm_indexes]\n",
    "\n",
    "    input = {\n",
    "        \"embeddings\": embeddings,\n",
    "        \"embeddings_lengths\": embeddings_lengths\n",
    "    }\n",
    "\n",
    "    return input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umkuMf60wA4M"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from skorch.callbacks import Callback\n",
    "\n",
    "def calculate_accuracy(net, dataset, y_true):\n",
    "    \"\"\"Calculates the accuracy of the model\"\"\"\n",
    "    y_pred = np.rint(net.predict(dataset))\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calculate_precision(net, dataset, y_true):\n",
    "    \"\"\"Calculates the precision of the model\"\"\"\n",
    "    y_pred = net.predict(dataset)\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calculate_recall(net, dataset, y_true):\n",
    "    \"\"\"Calculates the recall of the model\"\"\"\n",
    "    y_pred = net.predict(dataset)\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return precision_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calculate_f1_score(net, dataset, y_true):\n",
    "    \"\"\"Calculates the F1 score of the model\"\"\"\n",
    "    y_pred = net.predict(dataset)\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return recall_score(y_true, y_pred)\n",
    "\n",
    "class HiPlotLog(Callback):\n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        current = net.history[-1]\n",
    "        filename = DATASET_TO_USE.lower()\n",
    "\n",
    "        with open(f\"output/{filename}.csv\", \"a\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\n",
    "                trait,\n",
    "                current[\"epoch\"],\n",
    "                parameters[\"learning_rate\"],\n",
    "                parameters[\"dropout_input\"],\n",
    "                parameters[\"dropout_output\"],\n",
    "                parameters[\"weight_decay\"],\n",
    "                parameters[\"batch_size\"],\n",
    "                parameters[\"hidden_dim\"],\n",
    "                EMBEDDINGS_MODEL,\n",
    "                \"BCE\",\n",
    "                \"sigmoid\",\n",
    "                current[\"train_loss\"],\n",
    "                current[\"valid_loss\"],\n",
    "                current[\"accuracy\"],\n",
    "                current[\"precision\"],\n",
    "                current[\"recall\"],\n",
    "                current[\"f1_score\"]\n",
    "            ])\n",
    "\n",
    "class SaveBestModel(Callback):\n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        current = net.history[-1]\n",
    "\n",
    "        if current[\"accuracy_best\"]:\n",
    "            net.save_params(f_params=f\"trained_models/{trait}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring, ProgressBar\n",
    "\n",
    "def train(parameters, trait, dataset):\n",
    "    \"\"\"Train the model and print the output\"\"\"\n",
    "    \n",
    "    print(\"Parameters\")\n",
    "    print(f\"Batch Size: {parameters['batch_size']}\")\n",
    "    print(f\"Learning Rate: {parameters['learning_rate']}\")\n",
    "    print(f\"Max Epochs: {parameters['max_epochs']}\")\n",
    "    print(f\"Input Dropout: {parameters['dropout_input']}\")\n",
    "    print(f\"Output Dropout: {parameters['dropout_output']}\")\n",
    "    print(f\"Weight Decay: {parameters['weight_decay']}\")\n",
    "    print(f\"Hidden Dim: {parameters['hidden_dim']}\")\n",
    "    print(f\"Cross Validation Split: {parameters['cross_validation_split']}\")\n",
    "    print(f\"Embeddings Model: {EMBEDDINGS_MODEL}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(f\"Training {trait}:\")\n",
    "\n",
    "    net = NeuralNet(\n",
    "        module=LstmModel,\n",
    "        module__dropout_input=parameters[\"dropout_input\"],\n",
    "        module__dropout_output=parameters[\"dropout_output\"],\n",
    "        module__hidden_dim=parameters[\"hidden_dim\"],\n",
    "        criterion=nn.BCELoss,\n",
    "        optimizer=optim.Adam,\n",
    "        optimizer__weight_decay=parameters[\"weight_decay\"],\n",
    "        optimizer__lr=parameters[\"learning_rate\"],\n",
    "        iterator_train__collate_fn=collate_fn,\n",
    "        iterator_valid__collate_fn=collate_fn,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_valid__shuffle=True,\n",
    "        max_epochs=parameters[\"max_epochs\"],\n",
    "        batch_size=parameters[\"batch_size\"],\n",
    "        train_split=skorch.dataset.CVSplit(parameters[\"cross_validation_split\"], stratified=True, random_state=0),\n",
    "        callbacks=[\n",
    "            EpochScoring(calculate_accuracy, name=\"accuracy\", lower_is_better=False),\n",
    "            EpochScoring(calculate_precision, name=\"precision\", lower_is_better=False),\n",
    "            EpochScoring(calculate_recall, name=\"recall\", lower_is_better=False),\n",
    "            EpochScoring(calculate_f1_score, name=\"f1_score\", lower_is_better=False),\n",
    "            EpochScoring(calculate_f1_score, name=\"f1_score\", lower_is_better=False),\n",
    "            HiPlotLog(),\n",
    "            SaveBestModel(),\n",
    "            ProgressBar(),\n",
    "        ],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Get labels for a specific trait\n",
    "    dataset.set_trait(trait)\n",
    "    y = dataset.get_labels()\n",
    "\n",
    "    net.fit(dataset, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n",
      "Batch Size: 128\n",
      "Learning Rate: 0.0001\n",
      "Max Epochs: 1\n",
      "Input Dropout: 0.2\n",
      "Output Dropout: 0\n",
      "Weight Decay: 0.001\n",
      "Hidden Dim: 192\n",
      "Cross Validation Split: 10\n",
      "Embeddings Model: distilroberta-base\n",
      "\n",
      "\n",
      "Training cEXT:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aida/server/aida/user_analysis/model/Attention.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_score = torch.nn.functional.softmax(attention_score).view(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    accuracy    f1_score    precision    recall    train_loss    valid_loss     dur\n",
      "-------  ----------  ----------  -----------  --------  ------------  ------------  ------\n",
      "      1      \u001b[36m0.5101\u001b[0m      \u001b[32m0.9921\u001b[0m       \u001b[35m0.6756\u001b[0m    \u001b[31m0.5122\u001b[0m        \u001b[94m0.6952\u001b[0m        \u001b[36m0.6929\u001b[0m  4.9193\n",
      "Parameters\n",
      "Batch Size: 128\n",
      "Learning Rate: 0.0001\n",
      "Max Epochs: 3\n",
      "Input Dropout: 0.2\n",
      "Output Dropout: 0\n",
      "Weight Decay: 0.001\n",
      "Hidden Dim: 192\n",
      "Cross Validation Split: 10\n",
      "Embeddings Model: roberta-base\n",
      "\n",
      "\n",
      "Training cOPN:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07bb2763a43433ca9a9393217581106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aida/server/aida/user_analysis/model/Attention.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_score = torch.nn.functional.softmax(attention_score).view(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n",
      "Batch Size: 128\n",
      "Learning Rate: 0.0001\n",
      "Max Epochs: 91\n",
      "Input Dropout: 0.2\n",
      "Output Dropout: 0\n",
      "Weight Decay: 0.001\n",
      "Hidden Dim: 192\n",
      "Cross Validation Split: 10\n",
      "Embeddings Model: bert-base-uncased\n",
      "\n",
      "\n",
      "Training cNEU:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6785e248fe8845d1962c10f2463392eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aida/server/aida/user_analysis/model/Attention.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_score = torch.nn.functional.softmax(attention_score).view(\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"max_epochs\": 100,\n",
    "    \"batch_size\": 128,\n",
    "    \"hidden_dim\": 192,\n",
    "    \"dropout_input\": 0.2,\n",
    "    \"dropout_output\": 0,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"cross_validation_split\": 10\n",
    "}\n",
    "\n",
    "# Agreeableness\n",
    "# trait = \"cAGR\"\n",
    "# EMBEDDINGS_MODEL = \"distilbert-base-uncased\"\n",
    "# dataset = load_dataset()\n",
    "# train({**parameters, \"max_epochs\": 46}, trait, dataset)\n",
    "\n",
    "# Conscientiousness\n",
    "# trait = \"cCON\"\n",
    "# EMBEDDINGS_MODEL = \"roberta-base\"\n",
    "# dataset = load_dataset()\n",
    "# train({**parameters, \"max_epochs\": 1}, trait, dataset)\n",
    "\n",
    "# Extroversion\n",
    "trait = \"cEXT\"\n",
    "EMBEDDINGS_MODEL = \"distilroberta-base\"\n",
    "train({**parameters, \"max_epochs\": 1}, trait, dataset)\n",
    "\n",
    "# Openness\n",
    "trait = \"cOPN\"\n",
    "EMBEDDINGS_MODEL = \"roberta-base\"\n",
    "train({**parameters, \"max_epochs\": 3}, trait, dataset)\n",
    "\n",
    "# Neuroticism\n",
    "trait = \"cNEU\"\n",
    "EMBEDDINGS_MODEL = \"bert-base-uncased\"\n",
    "train({**parameters, \"max_epochs\": 91}, trait, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hiplot as hip\n",
    "\n",
    "filename = DATASET_TO_USE.lower()\n",
    "with open(f\"output/{filename}.csv\") as file:\n",
    "    experiment = hip.Experiment.from_csv(file)\n",
    "    experiment.parameters_definition[\"accuracy\"].force_range(0.45, 0.75)\n",
    "    experiment.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
