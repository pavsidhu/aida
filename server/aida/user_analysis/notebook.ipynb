{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality Analysis using a Bimodel LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install torch skorch transformers pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skorch\n",
    "import torch\n",
    "\n",
    "from IPython.display import display\n",
    "from skorch import NeuralNet\n",
    "from torch import nn, optim, tensor\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "from model.LstmModel import LstmModel\n",
    "from utils import progress_bar\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the MyPersonality dataset\n",
    "MY_PERSONALITY_PATH = \"data/mypersonality.csv\"\n",
    "\n",
    "# Path to the Essays dataset\n",
    "ESSAYS_PATH = \"data/essays.csv\"\n",
    "\n",
    "# List of traits to analyse\n",
    "TRAITS = [\"cAGR\", \"cCON\", \"cEXT\", \"cOPN\", \"cNEU\"]\n",
    "\n",
    "# Max length of tokens when calculating embeddings\n",
    "MAX_LENGTH = 400\n",
    "\n",
    "# Specify what dataset to use, can be either ESSAYS or MY_PERSONALITY\n",
    "DATASET_TO_USE = \"ESSAYS\"\n",
    "\n",
    "# Specify was model from the Transformers library to use to calculate embeddings\n",
    "EMBEDDINGS_MODEL = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "from skorch.helper import SliceDict\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from utils import progress_bar\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    \"\"\"Generate word embeddings for all the texts\"\"\"\n",
    "\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress = display(progress_bar(0, 100), display_id=True)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(EMBEDDINGS_MODEL)\n",
    "\n",
    "        embeddings_model = AutoModel.from_pretrained(EMBEDDINGS_MODEL).to(device)\n",
    "        embeddings_model.eval()\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            encoded_text = tokenizer.encode_plus(\n",
    "                text.lower(),\n",
    "                add_special_tokens=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                pad_to_max_length=True,\n",
    "            )\n",
    "            input_ids = encoded_text[\"input_ids\"]\n",
    "            attention_mask = encoded_text[\"attention_mask\"]\n",
    "\n",
    "            input = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "            input_mask = torch.tensor(attention_mask).to(device).unsqueeze(0)\n",
    "\n",
    "            output = embeddings_model(input, attention_mask=input_mask)[0]\n",
    "            output = output.squeeze().to(\"cpu\")\n",
    "\n",
    "            embeddings.append(output.numpy())\n",
    "\n",
    "            progress.update(progress_bar(i, len(texts)))\n",
    "\n",
    "    return np.stack(embeddings, axis=0)\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Loads a dataset and returns a PyTorch Dataset with embeddings\"\"\"\n",
    "\n",
    "    if DATASET_TO_USE == \"MY_PERSONALITY\":\n",
    "        path = MY_PERSONALITY_PATH\n",
    "        text_field = \"STATUS\"\n",
    "    elif DATASET_TO_USE == \"ESSAYS\":\n",
    "        path = ESSAYS_PATH\n",
    "        text_field = \"TEXT\"\n",
    "\n",
    "    df = pandas.read_csv(path, encoding=\"latin1\")\n",
    "    df[TRAITS] = df[TRAITS].replace(to_replace=[\"y\",\"n\"], value=[1.0,0.0])\n",
    "    df = df.rename(columns={text_field: \"TEXT\"})\n",
    "\n",
    "    embeddings = generate_embeddings(df[\"TEXT\"])\n",
    "    labels = {}\n",
    "\n",
    "    for trait in TRAITS:\n",
    "        labels[trait] = df[trait].to_numpy()\n",
    "    \n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umkuMf60wA4M"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from skorch.callbacks import Callback\n",
    "\n",
    "def scorer(y, y_pred):\n",
    "    \"\"\"Calculates the accuracy of the model\"\"\"\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return accuracy_score(y, y_pred)\n",
    "\n",
    "\n",
    "def calculate_accuracy(net, dataset, y_true):\n",
    "    \"\"\"Calculates the accuracy of the model\"\"\"\n",
    "    y_pred = np.rint(net.predict(dataset))\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calculate_precision(net, dataset, y_true):\n",
    "    \"\"\"Calculates the precision of the model\"\"\"\n",
    "    y_pred = net.predict(dataset)\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calculate_recall(net, dataset, y_true):\n",
    "    \"\"\"Calculates the recall of the model\"\"\"\n",
    "    y_pred = net.predict(dataset)\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return precision_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calculate_f1_score(net, dataset, y_true):\n",
    "    \"\"\"Calculates the F1 score of the model\"\"\"\n",
    "    y_pred = net.predict(dataset)\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return recall_score(y_true, y_pred)\n",
    "\n",
    "class HiPlotLog(Callback):\n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        current = net.history[-1]\n",
    "        filename = DATASET_TO_USE.lower()\n",
    "\n",
    "        with open(f\"output/{filename}.csv\", \"a\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\n",
    "                trait,\n",
    "                current[\"epoch\"],\n",
    "                parameters[\"learning_rate\"],\n",
    "                parameters[\"dropout_input\"],\n",
    "                parameters[\"dropout_output\"],\n",
    "                parameters[\"weight_decay\"],\n",
    "                parameters[\"batch_size\"],\n",
    "                parameters[\"hidden_dim\"],\n",
    "                EMBEDDINGS_MODEL,\n",
    "                \"BCE\",\n",
    "                \"sigmoid\",\n",
    "                current[\"train_loss\"],\n",
    "                current[\"valid_loss\"],\n",
    "                current[\"accuracy\"],\n",
    "                current[\"precision\"],\n",
    "                current[\"recall\"],\n",
    "                current[\"f1_score\"]\n",
    "            ])\n",
    "\n",
    "class SaveBestModel(Callback):\n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        current = net.history[-1]\n",
    "\n",
    "        if current[\"accuracy_best\"]:\n",
    "            filename = DATASET_TO_USE.lower()\n",
    "            \n",
    "            with open(f\"trained/best_{filename}.csv\", \"r+\") as file:\n",
    "                reader = csv.reader(file)\n",
    "                best_scores = list(reader)[-1]\n",
    "\n",
    "                index = TRAITS.index(trait)\n",
    "                best_score = best_scores[index]\n",
    "\n",
    "                if float(best_score) < current[\"accuracy\"]:\n",
    "                    with open(f\"trained/best_{filename}.csv\", \"a\") as file2:\n",
    "                            writer = csv.writer(file2)\n",
    "                            best_scores[index] = current[\"accuracy\"]\n",
    "                            writer.writerow(best_scores)\n",
    "                            net.save_params(f_params=f\"trained/{trait}.pt\")\n",
    "\n",
    "\n",
    "class FixRandomSeed(Callback):\n",
    "    def __init__(self, seed=0):\n",
    "        self.seed = 0\n",
    "    \n",
    "    def initialize(self):\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed(self.seed)\n",
    "        \n",
    "        try:\n",
    "            random.seed(self.seed)\n",
    "        except NameError:\n",
    "            import random\n",
    "            random.seed(self.seed)\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring, ProgressBar, EarlyStopping\n",
    "\n",
    "def create_net(parameters):\n",
    "    return NeuralNet(\n",
    "        module=LstmModel,\n",
    "        module__dropout_input=parameters[\"dropout_input\"],\n",
    "        module__hidden_dim=parameters[\"hidden_dim\"],\n",
    "        module__embedding_dim=parameters[\"embedding_dim\"],\n",
    "        criterion=nn.BCELoss,\n",
    "        optimizer=optim.Adam,\n",
    "        optimizer__weight_decay=parameters[\"weight_decay\"],\n",
    "        optimizer__lr=parameters[\"learning_rate\"],\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_valid__shuffle=True,\n",
    "        max_epochs=parameters[\"max_epochs\"],\n",
    "        batch_size=parameters[\"batch_size\"],\n",
    "        train_split=skorch.dataset.CVSplit(parameters[\"cross_validation_split\"], stratified=True, random_state=0),\n",
    "        callbacks=[\n",
    "            FixRandomSeed(parameters[\"seed\"]),\n",
    "            EpochScoring(calculate_accuracy, name=\"accuracy\", lower_is_better=False),\n",
    "            EpochScoring(calculate_precision, name=\"precision\", lower_is_better=False),\n",
    "            EpochScoring(calculate_recall, name=\"recall\", lower_is_better=False),\n",
    "            EpochScoring(calculate_f1_score, name=\"f1_score\", lower_is_better=False),\n",
    "            HiPlotLog(),\n",
    "            SaveBestModel(),\n",
    "            EarlyStopping(patience=40),\n",
    "            ProgressBar(),\n",
    "        ],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "def train(parameters, trait, X, y):\n",
    "    \"\"\"Train the model and print the output\"\"\"\n",
    "\n",
    "    print(f\"Training {trait}:\")\n",
    "    print(f\"  Batch Size: {parameters['batch_size']}\")\n",
    "    print(f\"  Learning Rate: {parameters['learning_rate']}\")\n",
    "    print(f\"  Max Epochs: {parameters['max_epochs']}\")\n",
    "    print(f\"  Input Dropout: {parameters['dropout_input']}\")\n",
    "    print(f\"  Output Dropout: {parameters['dropout_output']}\")\n",
    "    print(f\"  Weight Decay: {parameters['weight_decay']}\")\n",
    "    print(f\"  Hidden Dim: {parameters['hidden_dim']}\")\n",
    "    print(f\"  Cross Validation Split: {parameters['cross_validation_split']}\")\n",
    "    print(f\"  Embeddings Model: {EMBEDDINGS_MODEL}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    net = create_net(parameters)\n",
    "    net.fit(X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"learning_rate\": 0.00001,\n",
    "    \"max_epochs\": 200,\n",
    "    \"batch_size\": 128,\n",
    "    \"hidden_dim\": 192,\n",
    "    \"embedding_dim\": 768,\n",
    "    \"dropout_input\": 0.2,\n",
    "    \"dropout_output\": 0.1,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"cross_validation_split\": 10,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "embeddings, labels = load_dataset()\n",
    "\n",
    "for trait in TRAITS:\n",
    "        train(parameters, trait, embeddings, labels[trait])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import hiplot as hip\n",
    "\n",
    "def display_plot(filename):\n",
    "    with open(f\"output/{filename}.csv\") as file:\n",
    "        experiment = hip.Experiment.from_csv(file)\n",
    "        experiment.parameters_definition[\"accuracy\"].force_range(0.45, 0.75)\n",
    "        experiment.display()\n",
    "\n",
    "display_plot(\"essays\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
