{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality Analysis using a Bimodel LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: skorch in /home/ubuntu/.local/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.7/site-packages (2.5.1)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.7/site-packages (3.2.1)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in /home/ubuntu/.local/lib/python3.7/site-packages (from skorch) (4.43.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ubuntu/.local/lib/python3.7/site-packages (from skorch) (1.18.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/ubuntu/.local/lib/python3.7/site-packages (from skorch) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/ubuntu/.local/lib/python3.7/site-packages (from skorch) (0.22.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /home/ubuntu/.local/lib/python3.7/site-packages (from skorch) (0.8.6)\n",
      "Requirement already satisfied: sacremoses in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (2020.2.20)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: boto3 in /home/ubuntu/.local/lib/python3.7/site-packages (from transformers) (1.12.11)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ubuntu/.local/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ubuntu/.local/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ubuntu/.local/lib/python3.7/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/.local/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ubuntu/.local/lib/python3.7/site-packages (from scikit-learn>=0.19.1->skorch) (0.14.1)\n",
      "Requirement already satisfied: click in /home/ubuntu/.local/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers) (1.10.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/.local/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/.local/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/.local/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.11 in /home/ubuntu/.local/lib/python3.7/site-packages (from boto3->transformers) (1.15.11)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ubuntu/.local/lib/python3.7/site-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ubuntu/.local/lib/python3.7/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/.local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (46.1.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/lib/python3/dist-packages (from botocore<1.16.0,>=1.15.11->boto3->transformers) (0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch skorch transformers pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skorch\n",
    "import torch\n",
    "\n",
    "from IPython.display import display\n",
    "from skorch import NeuralNet\n",
    "from torch import nn, optim, tensor\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "from model.LstmModel import LstmModel\n",
    "from utils import progress_bar\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the MyPersonality dataset\n",
    "MY_PERSONALITY_PATH = \"data/mypersonality.csv\"\n",
    "\n",
    "# Path to the Essays dataset\n",
    "ESSAYS_PATH = \"data/essays.csv\"\n",
    "\n",
    "# List of traits to analyse\n",
    "TRAITS = [\"cAGR\", \"cCON\", \"cEXT\", \"cOPN\", \"cNEU\"]\n",
    "\n",
    "# Max length of tokens when calculating embeddings\n",
    "MAX_LENGTH = 250\n",
    "\n",
    "# Specify what dataset to use, can be either ESSAYS or MY_PERSONALITY\n",
    "DATASET_TO_USE = \"ESSAYS\"\n",
    "\n",
    "# Specify was model from the Transformers library to use to calculate embeddings\n",
    "EMBEDDINGS_MODEL = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "from skorch.helper import SliceDict\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from utils import progress_bar\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    \"\"\"Generate word embeddings for all the texts\"\"\"\n",
    "\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress = display(progress_bar(0, 100), display_id=True)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(EMBEDDINGS_MODEL)\n",
    "\n",
    "        embeddings_model = AutoModel.from_pretrained(EMBEDDINGS_MODEL).to(device)\n",
    "        embeddings_model.eval()\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            encoded_text = tokenizer.encode_plus(\n",
    "                text.lower(),\n",
    "                add_special_tokens=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                pad_to_max_length=True,\n",
    "            )\n",
    "            input_ids = encoded_text[\"input_ids\"]\n",
    "            attention_mask = encoded_text[\"attention_mask\"]\n",
    "\n",
    "            input = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "            input_mask = torch.tensor(attention_mask).to(device).unsqueeze(0)\n",
    "\n",
    "            output = embeddings_model(input, attention_mask=input_mask)[0]\n",
    "            output = output.squeeze().to(\"cpu\")\n",
    "\n",
    "            embeddings.append(output.numpy())\n",
    "\n",
    "            progress.update(progress_bar(i, len(texts)))\n",
    "\n",
    "    return np.stack(embeddings, axis=0)\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Loads a dataset and returns a PyTorch Dataset with embeddings\"\"\"\n",
    "\n",
    "    if DATASET_TO_USE == \"MY_PERSONALITY\":\n",
    "        path = MY_PERSONALITY_PATH\n",
    "        text_field = \"STATUS\"\n",
    "    elif DATASET_TO_USE == \"ESSAYS\":\n",
    "        path = ESSAYS_PATH\n",
    "        text_field = \"TEXT\"\n",
    "\n",
    "    df = pandas.read_csv(path, encoding=\"latin1\")\n",
    "    df[TRAITS] = df[TRAITS].replace(to_replace=[\"y\",\"n\"], value=[1.0,0.0])\n",
    "    df = df.rename(columns={text_field: \"TEXT\"})\n",
    "\n",
    "    embeddings = generate_embeddings(df[\"TEXT\"])\n",
    "    labels = {}\n",
    "\n",
    "    for trait in TRAITS:\n",
    "        labels[trait] = df[trait].to_numpy()\n",
    "    \n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umkuMf60wA4M"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from skorch.callbacks import Callback\n",
    "\n",
    "def scorer(y, y_pred):\n",
    "    \"\"\"Calculates the accuracy of the model\"\"\"\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return accuracy_score(y, y_pred)\n",
    "\n",
    "\n",
    "def calculate_accuracy(net, dataset, y_true):\n",
    "    \"\"\"Calculates the accuracy of the model\"\"\"\n",
    "    y_pred = np.rint(net.predict(dataset))\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calculate_precision(net, dataset, y_true):\n",
    "    \"\"\"Calculates the precision of the model\"\"\"\n",
    "    y_pred = net.predict(dataset)\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calculate_recall(net, dataset, y_true):\n",
    "    \"\"\"Calculates the recall of the model\"\"\"\n",
    "    y_pred = net.predict(dataset)\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return precision_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calculate_f1_score(net, dataset, y_true):\n",
    "    \"\"\"Calculates the F1 score of the model\"\"\"\n",
    "    y_pred = net.predict(dataset)\n",
    "    y_pred = np.rint(y_pred)\n",
    "    return recall_score(y_true, y_pred)\n",
    "\n",
    "class HiPlotLog(Callback):\n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        current = net.history[-1]\n",
    "        filename = DATASET_TO_USE.lower()\n",
    "\n",
    "        with open(f\"output/variance_{filename}_bert.csv\", \"a\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\n",
    "                trait,\n",
    "                current[\"epoch\"],\n",
    "                parameters[\"learning_rate\"],\n",
    "                parameters[\"dropout_input\"],\n",
    "                parameters[\"dropout_output\"],\n",
    "                parameters[\"weight_decay\"],\n",
    "                parameters[\"batch_size\"],\n",
    "                parameters[\"hidden_dim\"],\n",
    "                EMBEDDINGS_MODEL,\n",
    "                \"BCE\",\n",
    "                \"sigmoid\",\n",
    "                current[\"train_loss\"],\n",
    "                current[\"valid_loss\"],\n",
    "                current[\"accuracy\"],\n",
    "                current[\"precision\"],\n",
    "                current[\"recall\"],\n",
    "                current[\"f1_score\"]\n",
    "            ])\n",
    "\n",
    "class SaveBestModel(Callback):\n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        current = net.history[-1]\n",
    "\n",
    "        if current[\"accuracy_best\"]:\n",
    "            filename = DATASET_TO_USE.lower()\n",
    "            \n",
    "            with open(f\"trained/best_{filename}.csv\", \"r+\") as file:\n",
    "                reader = csv.reader(file)\n",
    "                best_scores = list(reader)[-1]\n",
    "\n",
    "                index = TRAITS.index(trait)\n",
    "                best_score = best_scores[index]\n",
    "\n",
    "                if float(best_score) < current[\"accuracy\"]:\n",
    "                    with open(f\"trained/best_{filename}.csv\", \"a\") as file2:\n",
    "                            writer = csv.writer(file2)\n",
    "                            best_scores[index] = current[\"accuracy\"]\n",
    "                            writer.writerow(best_scores)\n",
    "                            net.save_params(f_params=f\"trained/{trait}.pt\")\n",
    "\n",
    "\n",
    "class FixRandomSeed(Callback):\n",
    "    def __init__(self, seed=0):\n",
    "        self.seed = 0\n",
    "    \n",
    "    def initialize(self):\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed(self.seed)\n",
    "        \n",
    "        try:\n",
    "            random.seed(self.seed)\n",
    "        except NameError:\n",
    "            import random\n",
    "            random.seed(self.seed)\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring, ProgressBar, EarlyStopping\n",
    "\n",
    "def create_net(parameters):\n",
    "    return NeuralNet(\n",
    "        module=LstmModel,\n",
    "        module__dropout_input=parameters[\"dropout_input\"],\n",
    "        module__dropout_output=parameters[\"dropout_output\"],\n",
    "        module__hidden_dim=parameters[\"hidden_dim\"],\n",
    "        module__embedding_dim=parameters[\"embedding_dim\"],\n",
    "        criterion=nn.BCELoss,\n",
    "        optimizer=optim.Adam,\n",
    "        optimizer__weight_decay=parameters[\"weight_decay\"],\n",
    "        optimizer__lr=parameters[\"learning_rate\"],\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_valid__shuffle=True,\n",
    "        max_epochs=parameters[\"max_epochs\"],\n",
    "        batch_size=parameters[\"batch_size\"],\n",
    "        train_split=skorch.dataset.CVSplit(parameters[\"cross_validation_split\"], stratified=True, random_state=0),\n",
    "        callbacks=[\n",
    "#             FixRandomSeed(parameters[\"seed\"]),\n",
    "            EpochScoring(calculate_accuracy, name=\"accuracy\", lower_is_better=False),\n",
    "            EpochScoring(calculate_precision, name=\"precision\", lower_is_better=False),\n",
    "            EpochScoring(calculate_recall, name=\"recall\", lower_is_better=False),\n",
    "            EpochScoring(calculate_f1_score, name=\"f1_score\", lower_is_better=False),\n",
    "            HiPlotLog(),\n",
    "            SaveBestModel(),\n",
    "            EarlyStopping(patience=40),\n",
    "            ProgressBar(),\n",
    "        ],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "def train(parameters, trait, X, y):\n",
    "    \"\"\"Train the model and print the output\"\"\"\n",
    "\n",
    "    print(f\"Training {trait}:\")\n",
    "    print(f\"  Batch Size: {parameters['batch_size']}\")\n",
    "    print(f\"  Learning Rate: {parameters['learning_rate']}\")\n",
    "    print(f\"  Max Epochs: {parameters['max_epochs']}\")\n",
    "    print(f\"  Input Dropout: {parameters['dropout_input']}\")\n",
    "    print(f\"  Output Dropout: {parameters['dropout_output']}\")\n",
    "    print(f\"  Weight Decay: {parameters['weight_decay']}\")\n",
    "    print(f\"  Hidden Dim: {parameters['hidden_dim']}\")\n",
    "    print(f\"  Cross Validation Split: {parameters['cross_validation_split']}\")\n",
    "    print(f\"  Embeddings Model: {EMBEDDINGS_MODEL}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    net = create_net(parameters)\n",
    "    net.fit(X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "    \"learning_rate\": 0.00001,\n",
    "    \"max_epochs\": 200,\n",
    "    \"batch_size\": 128,\n",
    "    \"hidden_dim\": 192,\n",
    "    \"embedding_dim\": 768,\n",
    "    \"dropout_input\": 0.2,\n",
    "    \"dropout_output\": 0.1,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"cross_validation_split\": 10,\n",
    "}\n",
    "\n",
    "# EMBEDDINGS_MODEL = \"roberta-base\"\n",
    "# embeddings, labels = load_dataset()\n",
    "\n",
    "# for x in range(0, 50):\n",
    "#     for trait in TRAITS:\n",
    "#         print(f\"Iteration: {x}\")\n",
    "#         train(parameters, trait, embeddings, labels[trait])\n",
    "\n",
    "EMBEDDINGS_MODEL = \"bert-base-uncased\"\n",
    "embeddings, labels = load_dataset()\n",
    "\n",
    "for x in range(0, 12):\n",
    "    for trait in TRAITS:\n",
    "        print(f\"Iteration: {x}\")\n",
    "        train(parameters, trait, embeddings, labels[trait])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from skorch.helper import SliceDataset\n",
    "\n",
    "parameters = {\n",
    "    \"learning_rate\": 0.00001,\n",
    "    \"max_epochs\": 150,\n",
    "    \"batch_size\": 128,\n",
    "    \"hidden_dim\": 192,\n",
    "    \"embedding_dim\": 768,\n",
    "    \"dropout_input\": 0.2,\n",
    "    \"dropout_output\": 0.1,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"cross_validation_split\": 10,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "grid_search_parameters = {\n",
    "    \"module__dropout_input\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"module__dropout_output\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"optimizer__weight_decay\": [0.00001, 0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "EMBEDDINGS_MODEL = \"roberta-base\"\n",
    "embeddings, labels = load_dataset()\n",
    "\n",
    "for trait in TRAITS:\n",
    "    net = create_net(parameters)\n",
    "\n",
    "    grid_searc\n",
    "    h = RandomizedSearchCV(net, grid_search_parameters, cv=parameters[\"cross_validation_split\"], scoring=make_scorer(scorer))\n",
    "    grid_search.fit(embeddings, labels[trait])\n",
    "\n",
    "    print(grid_search.best_score_, grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import hiplot as hip\n",
    "\n",
    "def display_plot(filename):\n",
    "    with open(f\"output/{filename}.csv\") as file:\n",
    "        experiment = hip.Experiment.from_csv(file)\n",
    "        experiment.parameters_definition[\"accuracy\"].force_range(0.45, 0.75)\n",
    "        experiment.display()\n",
    "\n",
    "display_plot(\"mypersonality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
